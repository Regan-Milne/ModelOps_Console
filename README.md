ğŸš€ ModelOps Console

A lightweight, production-style MLOps observability stack for local Ollama models.

This project demonstrates how to build a real inference service, expose structured Prometheus metrics, and visualize live model performance through Grafana â€” all wired together with Docker Compose.

Itâ€™s deliberately simple, transparent, and easy to extend, making it ideal for:

developers learning MLOps fundamentals

teams who want observability around local LLM experiments

founders prototyping model evaluation workflows

engineers exploring inference telemetry or model routing

ğŸ“Š What This Project Shows (in Plain English)

This repo is not â€œjust a dashboard.â€
Itâ€™s a minimal, but real, MLOps pipeline:

1. A model server that actually produces metrics

FastAPI wraps a local Ollama model and logs:

request counts

latency histograms

token throughput

error rates

model names / statuses

2. Prometheus scrapes & stores the metrics

You get real, queryable time-series data.

3. Grafana visualizes everything in real-time

Dashboards show the health + behavior of your model:

token throughput by model (prioritized as primary KPI)

request rate

response latency (95th percentile)

error rate

4. Everything runs locally with one command

No GPU required.
No cloud.
Just Docker + Ollama.

This repo is intentionally small, auditable, and production-shaped.

ğŸ¥ Live Demo (GIF)

(Insert your new GIF here once you upload it:)

![demo](docs/demo.gif)


Nothing explains an MLOps system better than seeing the metrics react in real time.

ğŸ§± Architecture Overview

Hereâ€™s the blueprint:

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚    Client / UI       â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  FastAPI Chat     â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚   Ollama Model        â”‚
 â”‚  (ModelOps Console)  â”‚       â”‚  Service          â”‚        â”‚   (local inference)   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚                                 â”‚
            â”‚ emits /metrics                  â”‚ generates tokens
            â–¼                                 â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚     Prometheus       â”‚â—€â”€â”€â”€â”€â”€â”€â”‚  chat-service /metrics        â”‚
 â”‚  (scraping engine)   â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚       Grafana        â”‚
 â”‚ (dashboards + alerts)â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


This is the same architecture used in production ML systems â€” just scaled down for local experimentation.

âš¡ Quick Start
1. Start Ollama
ollama serve
ollama pull phi3:mini

2. Run the entire stack
docker compose up --build

3. Explore the services
Service	URL
Chat API (FastAPI)	http://localhost:8000

API Docs	http://localhost:8000/docs

Metrics Endpoint	http://localhost:8000/metrics

Prometheus	http://localhost:9090

Grafana	http://localhost:3000
 (admin/admin)
ğŸ§ª Try a Chat Request
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "What can you do?"}'


Or choose a different model:

curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "hello", "model": "qwen3:14b"}'

ğŸ“ˆ Metrics Tracked
Core Prometheus metrics:

chat_requests_total

chat_request_latency_seconds_bucket

chat_tokens_total

chat_errors_total

Grafana dashboard panels (in order):
# Token Throughput by Model (primary KPI)
sum by(model) (rate(chat_tokens_total[10s]))

# Chat Request Rate
rate(chat_requests_total[10s])

# Response Latency (95th percentile)
histogram_quantile(0.95, sum by (le) (rate(chat_request_latency_seconds_bucket[20s])))

# Error Rate
(
  sum(rate(chat_requests_total{status_code!="200"}[5m]))
/
  sum(rate(chat_requests_total[5m]))
) * 100
OR
vector(0)


This mirrors real-world ML observability: rate, latency, throughput, and errors.

ğŸ›  Configuration

Environment Variables:

DEFAULT_MODEL=phi3:mini
OLLAMA_BASE_URL=http://host.docker.internal:11434
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin


To change the default model:

environment:
  - DEFAULT_MODEL=qwen3:14b

ğŸ— Project Structure
ModelOps_Console/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ STARTUP_PROCEDURE.md         # Complete setup and troubleshooting guide
â”œâ”€â”€ PROJECT_LOG.md              # Development history and technical notes
â”œâ”€â”€ chat-service/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py             # FastAPI service with metrics
â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”‚       â””â”€â”€ chat.html       # Web UI with embedded Grafana panels
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ prometheus/
â”‚   â””â”€â”€ prometheus.yml
â””â”€â”€ grafana/
    â””â”€â”€ dashboard.json           # Pre-configured Ollama metrics dashboard

ğŸ› Troubleshooting
Prometheus shows no metrics

Check http://localhost:8000/metrics

Ensure chat_requests_total increments when sending messages

Verify Prometheus prometheus.yml has the correct target

Grafana panel blank or showing the home page

Use /d-solo/ embed URLs with &kiosk parameter for individual panels

Ensure panelId= matches dashboard.json (1=Chat Rate, 2=Latency, 3=Tokens, 4=Errors)

Example working URL: http://localhost:3000/d-solo/chat-metrics/ollama-chat-metrics?orgId=1&panelId=3&theme=dark&from=now-5m&to=now&refresh=5s&kiosk

Ollama not reachable
curl http://localhost:11434/api/tags


If this fails â†’ Ollama isnâ€™t running.

ğŸ§­ Why This Project Exists

Many MLOps tutorials focus on:

fancy models

cloud deployments

abstract concepts

But modern ML systems live or die by observability.

This project teaches the essentials:

how to serve a model

how to instrument it

how to expose metrics

how to scrape metrics

how to visualize performance

how to debug latency + throughput

Itâ€™s a small but realistic example of production-style ML telemetry that works on any laptop.

ğŸ™Œ Contributing

Feel free to open issues or PRs:

new panels

additional metrics

multi-model routing

benchmarking scripts

GPU inference support

ğŸ“„ License

MIT License â€” use it freely.